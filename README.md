# Self-Hosted Neon on AWS EKS

Automated deployment of [Neon](https://github.com/neondatabase/neon) (serverless Postgres) on AWS EKS, built from source on Fedora Linux.

**Region:** us-west-2 | **Prefix:** `smohan-neon1` | **Host OS:** Fedora 43

> **STATUS: Work in progress.** The data plane (pageserver, safekeepers, broker, proxy, storage-controller) builds and deploys. Compute nodes (Postgres) can be spun up per-tenant via `06-spin-compute.sh`. See [Known Gaps / TODO](#known-gaps--todo).

---

## Table of Contents

1. [What This Is](#what-this-is)
2. [Neon Architecture](#neon-architecture)
3. [Repository Layout](#repository-layout)
4. [Prerequisites](#prerequisites)
5. [Pipeline Overview](#pipeline-overview)
6. [Running the Deployment](#running-the-deployment)
7. [Teardown](#teardown)
8. [Build Notes & Lessons Learned](#build-notes--lessons-learned)
9. [AWS Infrastructure Details](#aws-infrastructure-details)
10. [Kubernetes Manifests](#kubernetes-manifests)
11. [Sizing & Cost](#sizing--cost)
12. [Verification & Debugging](#verification--debugging)
13. [Control Plane (Future Work)](#control-plane-future-work)
14. [Known Gaps / TODO](#known-gaps--todo)
15. [Quick Reference](#quick-reference)

---

## What This Is

A set of shell scripts that:

1. Build all Neon Rust binaries from source on a Fedora workstation
2. Package them into Docker images
3. Stand up AWS infrastructure (EKS, S3, ECR, IAM)
4. Push images and deploy to Kubernetes

The goal is learning and experimentation — understand Neon's architecture hands-on, then eventually migrate to corporate Kubernetes for production.

### What Neon Does

- Separation of storage and compute for Postgres
- Instant database branching (copy-on-write)
- Point-in-time recovery
- S3-backed durable storage with local caching
- Serverless compute (auto-suspend/resume) — requires control plane (proprietary)

### The Challenge

Neon's **data plane** is open source. The **control plane** (tenant management, branch provisioning, compute orchestration) is proprietary. This project deploys the data plane; control plane is future work.

---

## Neon Architecture

### Core Components (All Open Source)

| Component | Language | Role | Ports |
|---|---|---|---|
| **Pageserver** | Rust | Storage engine — serves pages to compute, stores data in S3 with local cache | 6400 (pg), 9898 (http) |
| **Safekeeper** | Rust | WAL acceptor — Paxos consensus, 3+ nodes for HA quorum | 5454 (pg), 7676 (http) |
| **Storage Broker** | Rust | Coordinator — safekeeper discovery and health | 50051 (gRPC) |
| **Proxy** | Rust | Connection pooler/router — directs traffic to compute nodes | 4432 (pg), 7000 (http) |
| **Storage Controller** | Rust | Manages pageserver sharding and tenant placement | 6060, 9898 |
| **Compute Node** | Modified Postgres | Standard Postgres with Neon storage manager — reads from pageserver | 5432 |

### Data Flow

```
Client → Proxy → Compute Node (Postgres)
                      ↓ page requests
                  Pageserver
                      ↓ WAL
                  Safekeeper Quorum (3 nodes)
                      ↓ durable storage
                  S3 Object Storage
```

### What's Proprietary (NOT in the open-source repo)

- Control plane API (tenant/branch/compute lifecycle)
- Web console and UI
- Autoscaling and compute orchestration
- Multi-tenancy at scale
- Monitoring integration

---

## Repository Layout

```
setup_neon/
├── setup-neon-aws.sh               # Master orchestrator (runs phases 0-5)
├── 00-prerequisites.sh             # Fedora packages, Rust, Docker, AWS CLI, kubectl, eksctl
├── 01-build-neon.sh                # Build Neon Rust binaries + postgres extensions
├── 02-build-images.sh              # Build Docker images locally (no AWS needed)
├── 03-create-aws-infra.sh          # EKS cluster, S3, VPC endpoint, IAM/IRSA, ECR
├── 04-push-images.sh               # Tag and push images to ECR
├── 05-deploy-neon.sh               # Apply K8s manifests to EKS
├── 06-spin-compute.sh              # Create tenant + timeline, deploy a compute pod
├── 06-teardown-compute.sh          # Tear down compute pods (single, all, or list)
├── 99-teardown.sh                  # Destroy ALL AWS resources (reads .env)
├── create_iam_user.sh              # One-time: create IAM user for deployment
├── test_docker.sh                  # Smoke-test locally built Docker images
├── .env                            # Generated by 03 — resource IDs (gitignored)
├── .docker-staging/                # Temp binary staging for Docker builds (gitignored)
├── config/
│   ├── cluster-config.yaml         # eksctl ClusterConfig
│   └── pageserver.toml             # Pageserver config (reference)
├── dockerfiles/
│   ├── Dockerfile.pageserver       # fedora:43 base
│   ├── Dockerfile.safekeeper       # fedora:43 base
│   ├── Dockerfile.proxy            # fedora:43 base
│   ├── Dockerfile.storage-broker   # fedora:43 base
│   ├── Dockerfile.storage-controller  # fedora:43 base
│   └── Dockerfile.compute            # fedora:43 base, PostgreSQL v17 + neon extensions + compute_ctl
├── manifests/
│   ├── namespace.yaml              # neon namespace
│   ├── storage-classes.yaml        # gp3-encrypted StorageClass
│   ├── storage-broker/
│   │   └── deployment.yaml         # Deployment + ClusterIP Service
│   ├── safekeeper/
│   │   └── statefulset.yaml        # StatefulSet (3 replicas) + headless Service + PVCs
│   ├── pageserver/
│   │   └── statefulset.yaml        # StatefulSet (2 replicas) + headless Service + PVCs + SA + ConfigMap
│   └── proxy/
│       └── deployment.yaml         # Deployment + LoadBalancer Service
├── SIZING.md                       # Detailed cost/sizing breakdown
└── conv.md                         # Original architecture discussion notes
```

---

## Prerequisites

**Host machine:** Fedora Linux (tested on Fedora 43) with at least 16GB RAM.

`00-prerequisites.sh` installs everything needed:

- Development tools (gcc, make, flex, bison, etc.)
- Library headers: readline-devel, openssl-devel, libicu-devel, libcurl-devel, libseccomp-devel, clang-devel, protobuf-devel, and more
- **Rust** via rustup
- **mold** linker (critical for low-memory machines — see [Build Notes](#build-notes--lessons-learned))
- **Docker** (docker-ce from Docker's Fedora repo)
- **AWS CLI v2**
- **kubectl**
- **eksctl**
- **jq**

### AWS Credentials

All scripts use environment variables — no `aws configure` anywhere.

```bash
export AWS_ACCESS_KEY_ID=<your-key>
export AWS_SECRET_ACCESS_KEY=<your-secret>
export AWS_DEFAULT_REGION=us-west-2   # defaults to us-west-2 if unset
```

To create a dedicated IAM user with the required permissions:

```bash
./create_iam_user.sh
```

This creates user `smohan_neon_access` with `AdministratorAccess` (eksctl needs broad permissions: EKS, EC2, CloudFormation, IAM, VPC, ASG, ELB, SSM, etc.).

---

## Pipeline Overview

The deployment is split into 7 phases. Phases 0-5 are idempotent — safe to re-run after partial failures. Phase 6 creates new compute instances on each run.

| Phase | Script | Time | AWS Needed? | What It Does |
|---|---|---|---|---|
| 0 | `00-prerequisites.sh` | ~10 min | No | Install all build tools |
| 1 | `01-build-neon.sh` | ~30 min | No | Build Rust binaries + postgres extensions + compute_ctl |
| 2 | `02-build-images.sh` | ~2 min | No | Build 6 Docker images locally |
| 3 | `03-create-aws-infra.sh` | ~20 min | Yes | EKS, S3, VPC endpoint, IAM, ECR |
| 4 | `04-push-images.sh` | ~5 min | Yes | Push images to ECR |
| 5 | `05-deploy-neon.sh` | ~5 min | Yes | Deploy data plane (broker, safekeepers, pageserver, proxy) |
| 6 | `06-spin-compute.sh` | ~1 min | Yes | Create tenant + timeline, deploy a compute pod |

Key design decision: Docker images are built **before** AWS infrastructure. This lets you verify images work locally (`test_docker.sh`) before spending 20 minutes on EKS cluster creation.

### State File: `.env`

`03-create-aws-infra.sh` writes all generated resource IDs to `.env`:

```
PREFIX=smohan-neon1
REGION=us-west-2
ACCOUNT_ID=123456789012
CLUSTER_NAME=smohan-neon1-cluster
S3_BUCKET=smohan-neon1-pageserver-data
VPC_ID=vpc-xxx
VPC_ENDPOINT_ID=vpce-xxx
IAM_POLICY_ARN=arn:aws:iam::123456789012:policy/smohan-neon1-pageserver-s3
OIDC_PROVIDER=oidc.eks.us-west-2.amazonaws.com/id/xxx
ECR_REGISTRY=123456789012.dkr.ecr.us-west-2.amazonaws.com
```

Subsequent scripts source `.env`. Teardown reads `.env` to know what to delete.

---

## Running the Deployment

### Option A: Run Everything

```bash
export AWS_ACCESS_KEY_ID=<key>
export AWS_SECRET_ACCESS_KEY=<secret>
./setup-neon-aws.sh
```

This runs all 6 phases in sequence with confirmation prompts.

### Option B: Step by Step

```bash
# Phase 0: Install tools (skip if already done)
./00-prerequisites.sh

# Phase 1: Build Neon (from existing checkout at ~/SourceCode/neon)
./01-build-neon.sh

# Phase 2: Build Docker images (no AWS needed)
./02-build-images.sh

# Smoke test images locally
./test_docker.sh

# Phase 3: Create AWS infrastructure
export AWS_ACCESS_KEY_ID=<key>
export AWS_SECRET_ACCESS_KEY=<secret>
./03-create-aws-infra.sh

# Phase 4: Push to ECR
./04-push-images.sh

# Phase 5: Deploy data plane to EKS
./05-deploy-neon.sh

# Phase 6: Spin up a compute node (Postgres)
./06-spin-compute.sh
```

### After Deployment

```bash
kubectl get pods -n neon           # All pods should be Running
kubectl get svc -n neon            # Check services and LoadBalancer
```

### Connecting to Postgres

After `06-spin-compute.sh` completes, it prints the compute pod name (e.g., `compute-a1b2c3d4`):

```bash
# Port-forward from your machine
kubectl port-forward -n neon pod/compute-a1b2c3d4 5432:5432

# Connect with psql
psql postgresql://postgres@localhost:5432/postgres
```

### Managing Compute Nodes

```bash
# List running compute pods
./06-teardown-compute.sh --list

# Spin up another compute (new tenant + database)
./06-spin-compute.sh

# Tear down a specific compute
./06-teardown-compute.sh compute-a1b2c3d4

# Tear down ALL compute pods (prompts for confirmation)
./06-teardown-compute.sh --all
```

Tearing down a compute only removes the pod — tenant/timeline data remains on pageserver/S3. You can spin up a new compute pointing at the same data.

---

## Teardown

### Compute Nodes Only

```bash
./06-teardown-compute.sh --list     # See what's running
./06-teardown-compute.sh --all      # Remove all compute pods (data stays on pageserver)
```

### Full AWS Teardown

```bash
./99-teardown.sh
```

Confirms before proceeding. Destroys in reverse order:

1. K8s namespace (cascades to all workloads)
2. IRSA service account
3. VPC endpoint
4. EKS cluster (eksctl handles nodegroups, VPC, subnets, NAT, IGW)
5. ECR repositories (force, including all images)
6. S3 bucket (empties all versions + delete markers first)
7. IAM policy (detaches from roles first)
8. Removes `.env`

Re-runnable — skips resources that don't exist.

**Daily cost if left running: ~$8.75/day.** Always tear down when not in use.

---

## Build Notes & Lessons Learned

These are real issues encountered during the build on a Fedora 43 machine with 15GB RAM.

### Linker OOM / Segfault (GNU `ld`)

**Problem:** `collect2: fatal error: ld terminated with signal 11 [Segmentation fault]` when linking the `proxy` binary (pg_sni_router). GNU `ld` runs out of memory on machines with 16GB RAM.

**Root cause:** Fedora's default 8GB "swap" is **zram** (compressed RAM), not real disk swap. It doesn't provide actual additional memory for the linker.

**Fix:** Use the `mold` linker. `01-build-neon.sh` sets:
```bash
export RUSTFLAGS="-Cforce-frame-pointers=yes -Clink-arg=-fuse-ld=mold"
```

The `-Cforce-frame-pointers=yes` flag is preserved from neon's `.cargo/config.toml`. The RUSTFLAGS env var overrides config.toml — this was chosen over modifying neon's repo because changing rustflags invalidates all cargo fingerprints, causing a full rebuild.

**Install mold:** `sudo dnf install -y mold` (should be in `00-prerequisites.sh`).

### glibc Version Mismatch

**Problem:** Docker images fail with `GLIBC_2.39 not found`.

**Root cause:** Binaries built on Fedora 43 (glibc 2.39) can't run on Debian Bookworm (glibc 2.36). The base Docker image must match the build host's glibc version.

**Fix:** All Dockerfiles use `FROM fedora:43` instead of `debian:bookworm-slim`. AWS EKS doesn't care — it runs any OCI container image. The host kernel is shared; userspace is the container's own.

### Per-Package Cargo Builds

Building all 5 packages in one `cargo build` command risks OOM. `01-build-neon.sh` builds each package individually with `-j1`:

```bash
PACKAGES=(pageserver safekeeper proxy storage_broker storage_controller)
for pkg in "${PACKAGES[@]}"; do
    cargo build --release -j1 -p "${pkg}"
done
```

### Docker Group

After installing Docker, `usermod -aG docker $USER` requires re-login or `newgrp docker` to take effect in the current shell.

### Missing `libcurl-devel`

`neon-pg-ext-v17` needs `curl/curl.h`. Install with `sudo dnf install -y libcurl-devel`.

### Neon Source Directory

`01-build-neon.sh` expects an existing neon checkout at `/home/srinivas/SourceCode/neon`. It does NOT clone the repo — you must do that yourself:

```bash
git clone https://github.com/neondatabase/neon.git ~/SourceCode/neon
cd ~/SourceCode/neon && git submodule update --init --recursive
```

The build script also:
- Builds PostgreSQL v17 from `vendor/postgres-v17`
- Symlinks `pg_install/v14`, `v15`, `v16` to `v17` (postgres_ffi needs all versions)
- Builds the walproposer C library directly (bypasses jemalloc crash on low-memory machines)
- Builds `neon-pg-ext-v17` (postgres extensions for compute nodes)

---

## AWS Infrastructure Details

### What Gets Created

| Resource | Name/ID | Purpose |
|---|---|---|
| EKS Cluster | `smohan-neon1-cluster` | Managed K8s 1.32, 2x t3.large nodes |
| S3 Bucket | `smohan-neon1-pageserver-data` | Pageserver durable storage (versioned, lifecycle policies) |
| VPC Endpoint | Gateway type | S3 access without NAT egress charges |
| IAM Policy | `smohan-neon1-pageserver-s3` | S3 read/write scoped to the bucket |
| IRSA | `pageserver-sa` in namespace `neon` | Pods get S3 access via service account, no hardcoded keys |
| ECR Repos | `neon/{pageserver,safekeeper,proxy,storage-broker,storage-controller,compute}` | Container registry |
| StorageClass | `gp3-encrypted` | 16k IOPS, encrypted EBS volumes |

### EKS Cluster Config

- 2x `t3.large` nodes (2 vCPU, 8GB each) — burstable, fine for testing
- Private networking, gp3 root volumes
- OIDC enabled for IRSA
- Addons: vpc-cni, coredns, kube-proxy, aws-ebs-csi-driver

### S3 Configuration

- Versioning enabled
- Lifecycle: Intelligent Tiering after 30 days, non-current versions expire after 7 days

---

## Kubernetes Manifests

All manifests use `PLACEHOLDER_ECR_REGISTRY` and `PLACEHOLDER_S3_BUCKET` which are substituted by `05-deploy-neon.sh` at deploy time using values from `.env`.

### Deployment Order

1. **Namespace** (`neon`)
2. **StorageClass** (`gp3-encrypted`)
3. **Storage Broker** — Deployment (1 replica) + ClusterIP Service
4. **Safekeepers** — StatefulSet (3 replicas) + headless Service + 10Gi PVCs
5. **Pageserver** — StatefulSet (2 replicas) + headless Service + 20Gi PVCs + ServiceAccount + ConfigMap
6. **Proxy** — Deployment (1 replica) + LoadBalancer Service
7. **Compute** — Created on-demand by `06-spin-compute.sh` (Pod + ConfigMap + ClusterIP Service per tenant)

### Compute Nodes

Unlike the static data plane, compute pods are created dynamically. Each `06-spin-compute.sh` invocation:

1. Creates a tenant + timeline on pageserver (via HTTP API)
2. Generates a compute spec JSON (ConfigMap) with tenant/timeline IDs and cluster DNS addresses
3. Deploys a Pod running `compute_ctl` which launches PostgreSQL
4. Creates a ClusterIP Service for the pod

Compute pods are **stateless** — PGDATA uses `emptyDir`. All data lives in pageserver/S3. Destroying and recreating a compute pod for the same tenant/timeline picks up where it left off.

### Why StatefulSets

- Safekeepers need stable identities (`safekeeper-0`, `-1`, `-2`) for Paxos consensus
- Pageservers need persistent local cache that survives pod restarts
- Both get ordered startup/shutdown and stable DNS names

### Internal DNS

```
storage-broker.neon.svc.cluster.local:50051
safekeeper-{0,1,2}.safekeeper.neon.svc.cluster.local:5454
pageserver-{0,1}.pageserver.neon.svc.cluster.local:6400
```

---

## Sizing & Cost

See [SIZING.md](SIZING.md) for the full breakdown.

**Test setup (current):**
- 2x t3.large (2 vCPU, 8GB each)
- 7 pods total, ~2.25 CPU / ~4.25GB RAM requested
- ~180GB total disk (EBS + PVCs + S3)
- **~$263/month** (~$8.75/day)

**Production (future):**
- 3-5x m7i.4xlarge (16 vCPU, 64GB each)
- 32-64GB pageserver cache
- **~$2,500-3,500/month**

### Hidden Costs

- **NAT Gateway** (~$35/month): Required for private subnets to pull ECR images
- **Network Load Balancer** (~$18/month): For proxy service
- **t3.large burst credits**: Sustained CPU above 20% baseline consumes credits

---

## Verification & Debugging

### Check Deployment

```bash
kubectl get pods -n neon
kubectl get svc -n neon
kubectl describe pod pageserver-0 -n neon    # Check Events for errors
```

### Logs

```bash
kubectl logs -f pageserver-0 -n neon
kubectl logs -f safekeeper-0 -n neon
kubectl logs -f -l app=storage-broker -n neon
```

### Access Pageserver API

```bash
kubectl port-forward -n neon svc/pageserver 9898:9898
curl http://localhost:9898/v1/status
```

### Pageserver Tenant/Timeline API

```bash
# Create tenant
curl -X POST http://localhost:9898/v1/tenant \
  -H "Content-Type: application/json" \
  -d '{"tenant_id": "de200bd42b49cc1814412c7e592dd6e9"}'

# Create timeline (branch)
curl -X POST http://localhost:9898/v1/tenant/de200bd42b49cc1814412c7e592dd6e9/timeline \
  -H "Content-Type: application/json" \
  -d '{"new_timeline_id": "b3b845107e58ea5e3c0a67ef3fe24da2"}'

# List timelines
curl http://localhost:9898/v1/tenant/de200bd42b49cc1814412c7e592dd6e9/timeline
```

### Troubleshooting

**Pods not starting:** Check `kubectl describe pod <name> -n neon` — look at Events section for image pull errors, resource constraints.

**S3 access denied:** Verify IRSA annotation on service account:
```bash
kubectl get sa pageserver-sa -n neon -o yaml
# Should have: eks.amazonaws.com/role-arn annotation
```

**Safekeeper quorum issues:** All 3 safekeepers must be running. Check broker connectivity:
```bash
kubectl logs -l app=safekeeper -n neon | grep -i error
```

### Test Docker Images Locally

```bash
./test_docker.sh
```

Runs `--version` (or `--help` for broker) on each image. Note: proxy outputs JSON logs to stderr before the version string — this is normal.

### Verify Teardown

```bash
aws eks list-clusters --region us-west-2
aws s3 ls 2>&1 | grep smohan-neon1
aws ecr describe-repositories --region us-west-2
```

---

## Control Plane (Future Work)

The data plane (pageserver, safekeepers, broker, proxy) is what we deploy. The control plane (tenant management, compute lifecycle) is proprietary in managed Neon. Options for building our own:

### The Problem

In managed Neon, `neondb create-branch` does three things:
1. Creates a timeline on pageserver (open-source HTTP API)
2. Provisions a compute node (Postgres pod) — **proprietary**
3. Returns a connection string with DNS/routing — **proprietary**

### Recommended: Rust Kubernetes Operator

Using [kube-rs](https://github.com/kube-rs/kube):

1. Define a `NeonCompute` CRD
2. Operator watches CRD, creates Postgres Pods + Services
3. REST API layer for branch creation (calls pageserver API + creates CRD)

### Implementation Phases

1. **Manual testing** — Deploy a compute pod by hand, verify it connects to pageserver
2. **Basic operator** — CRD creates Pod + Service when applied
3. **Full API** — REST endpoint: create branch = create timeline + create compute
4. **Advanced** — Auto-suspend/resume, compute pooling, proxy-based routing

---

## Known Gaps / TODO

- **Storage controller**: Docker image builds, but no K8s manifest in `manifests/`.
- **`mold` not in `00-prerequisites.sh`**: Should add `sudo dnf install -y mold`.
- **Hardcoded neon source path**: `01-build-neon.sh` and `02-build-images.sh` hardcode `/home/srinivas/SourceCode/neon`.
- **Proxy configuration**: Currently starts with minimal args — needs auth configuration and compute routing for production use.
- **Monitoring**: No Prometheus/Grafana setup.
- **TLS**: No TLS termination configured.

---

## Quick Reference

### Cargo / Build

```bash
# Build specific component
cargo build --release -p pageserver

# Build with mold linker (for low-memory machines)
RUSTFLAGS="-Cforce-frame-pointers=yes -Clink-arg=-fuse-ld=mold" cargo build --release -j1 -p proxy

# Build postgres extensions
make neon-pg-ext-v17 -j1
```

### Docker

```bash
# List Neon images
docker images | grep neon

# Rebuild a single image
docker build -t neon/pageserver:latest -f dockerfiles/Dockerfile.pageserver .

# Where images are stored
ls /var/lib/docker/
```

### Kubernetes

```bash
kubectl get all -n neon
kubectl get pods -n neon -o wide
kubectl exec -it pageserver-0 -n neon -- /bin/bash
kubectl delete pod pageserver-0 -n neon    # StatefulSet recreates it
kubectl port-forward -n neon svc/pageserver 9898:9898
```

### AWS

```bash
# ── Identity & credentials ──
aws sts get-caller-identity                          # Verify which account/user you're using
aws iam list-access-keys --user-name smohan_neon_access  # List access keys for deploy user

# ── EKS cluster ──
aws eks list-clusters --region us-west-2             # List all clusters
aws eks describe-cluster --name smohan-neon1-cluster --region us-west-2  # Cluster details
aws eks update-kubeconfig --name smohan-neon1-cluster --region us-west-2  # Update kubeconfig
eksctl get cluster --region us-west-2                # eksctl view of clusters
eksctl get nodegroup --cluster smohan-neon1-cluster --region us-west-2   # Node group status

# ── ECR (container registry) ──
aws ecr describe-repositories --region us-west-2 --query "repositories[].repositoryName" --output table
aws ecr list-images --repository-name neon/pageserver --region us-west-2  # Images in a repo
aws ecr get-login-password --region us-west-2 | docker login --username AWS --password-stdin <ECR_REGISTRY>

# ── S3 ──
aws s3 ls 2>&1 | grep smohan-neon1                   # Check bucket exists
aws s3 ls s3://smohan-neon1-pageserver-data/ --recursive  # List bucket contents
aws s3api get-bucket-versioning --bucket smohan-neon1-pageserver-data  # Versioning status

# ── VPC & networking ──
aws ec2 describe-vpc-endpoints --region us-west-2 \
    --filters "Name=service-name,Values=com.amazonaws.us-west-2.s3" \
    --query "VpcEndpoints[].{ID:VpcEndpointId,State:State}" --output table

# ── IAM ──
aws iam get-policy --policy-arn arn:aws:iam::$(aws sts get-caller-identity --query Account --output text):policy/smohan-neon1-pageserver-s3
kubectl get sa pageserver-sa -n neon -o jsonpath='{.metadata.annotations}' # IRSA annotation

# ── CloudFormation (eksctl uses this under the hood) ──
aws cloudformation list-stacks --region us-west-2 \
    --query "StackSummaries[?contains(StackName,'smohan-neon1') && StackStatus!='DELETE_COMPLETE'].[StackName,StackStatus]" --output table

# ── Cost check ──
# No CLI for real-time cost — use AWS Cost Explorer in the console.
# Key billing items: EKS control plane ($0.10/hr), EC2 t3.large ($0.0832/hr each), NAT Gateway ($0.045/hr)
```

---

## Resources

- [Neon GitHub](https://github.com/neondatabase/neon)
- [Neon Architecture](https://neon.tech/docs/introduction/architecture)
- [kube-rs (Rust K8s client)](https://github.com/kube-rs/kube)
- [EKS Best Practices](https://aws.github.io/aws-eks-best-practices/)
- [IRSA Documentation](https://docs.aws.amazon.com/eks/latest/userguide/iam-roles-for-service-accounts.html)
